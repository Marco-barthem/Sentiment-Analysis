{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGSKT6XTSvm8bDSIa9Xitu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marco-barthem/Sentiment-Analysis/blob/main/SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLcI4zTe7fAe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "sns.set_style('darkgrid')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "yO-yFCiVKe4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/Marco-barthem/Sentiment-Analysis/refs/heads/main/ReviewsAmazonShort.csv')\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "Xbzb5r4O7pTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploration Data Analysis"
      ],
      "metadata": {
        "id": "jnugN9ZAye0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = df['Score'].value_counts().sort_index().plot(kind='bar',title='Score distribution',figsize=(10,5))\n",
        "plt.show()\n",
        "ax.set_xlabel('Score')\n",
        "ax.set_ylabel('Frequency')"
      ],
      "metadata": {
        "id": "UDJSO6_DynQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic NLTK"
      ],
      "metadata": {
        "id": "Ljp0IEZLzi9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example =  df['Text'][50]\n",
        "example"
      ],
      "metadata": {
        "id": "xpczu3MVzmnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "tokens_test = tokenizer.tokenize(example)\n",
        "tokens_test"
      ],
      "metadata": {
        "id": "VWF58WaR7wvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_test = nltk.pos_tag(tokens_test)\n",
        "tagged_test[:10]"
      ],
      "metadata": {
        "id": "L345XDlN71Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VADER - Valence Aware Dictionary and sEntiment Reasoner\n",
        "\n",
        "- **O que é VADER?**  \n",
        "  → É uma ferramenta de **análise de sentimentos** que utiliza um **dicionário** e um conjunto de regras de **pontuação** e **intensidade emocional** para determinar o **sentimento** de um texto.\n",
        "\n",
        "- **Nome:**  \n",
        "  → **VADER** significa **Valence Aware Dictionary and sEntiment Reasoner**.\n",
        "\n",
        "- **Objetivo:**  \n",
        "  → Realizar a **análise de sentimentos** de textos curtos, como **tweets**, resenhas e postagens em redes sociais, com uma abordagem **baseada em léxico**.\n",
        "\n",
        "- **Como funciona:**  \n",
        "  → VADER usa um dicionário de palavras com informações sobre seu **sentimento** (positivo, negativo, neutro) e ajusta a pontuação com base em regras como **capitalização**, **emoticons**, **grau de ênfase** e outros fatores contextuais.\n",
        "\n",
        "- **Diferenciais:**  \n",
        "  → É **específico** para textos curtos e **informais**, como **redes sociais**, diferente de outras abordagens como o **TextBlob** ou **Naive Bayes**, que podem não lidar tão bem com esse tipo de dado.\n",
        "\n",
        "- **Resultados:**  \n",
        "  → VADER atribui uma **pontuação de sentimento** que varia de -1 (negativo) a +1 (positivo), com 0 indicando neutralidade.\n",
        "\n",
        "- **Disponibilidade:**  \n",
        "  → O VADER está disponível no **nltk** e pode ser facilmente utilizado para tarefas de análise de sentimentos em textos em inglês.\n",
        "\n",
        "- **Aplicações:**  \n",
        "  → Ideal para análise de sentimentos em **redes sociais**, **resenhas de produtos**, **análises de marketing**, entre outros.\n"
      ],
      "metadata": {
        "id": "gj4qapCH3NL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "68P4DaGz1hKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(sia.polarity_scores(\"I hate this product\"))\n",
        "print(sia.polarity_scores(\"I Love this product\"))"
      ],
      "metadata": {
        "id": "xpoV-Gn-4Pj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Runing the polarity score in the dataset\n",
        "res ={}\n",
        "for i, row in tqdm(df.iterrows(),total=len(df)):\n",
        "    text = row['Text']\n",
        "    myid = row['Id']\n",
        "    res[myid] = sia.polarity_scores(text)"
      ],
      "metadata": {
        "id": "WZIv2PTY4ioq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_polarity_score = pd.DataFrame(res).T\n",
        "result_polarity_score = result_polarity_score.reset_index().rename(columns={'index':'Id'})\n",
        "df = df.merge(result_polarity_score,how='left')\n",
        "df"
      ],
      "metadata": {
        "id": "Wa0--PZu-zes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Vader Result"
      ],
      "metadata": {
        "id": "XDOC-fno_hMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.barplot(data=df, x='Score', y='compound')\n",
        "ax.set_title('Compund Score by Amazon Star Review')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aXmftxTx-0ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, #Row\n",
        "                        3, #Columns\n",
        "                        figsize=(12, 4))\n",
        "sns.barplot(data=df, x='Score', y='pos', ax=axs[0])\n",
        "sns.barplot(data=df, x='Score', y='neu', ax=axs[1])\n",
        "sns.barplot(data=df, x='Score', y='neg', ax=axs[2])\n",
        "axs[0].set_title('Positive')\n",
        "axs[1].set_title('Neutral')\n",
        "axs[2].set_title('Negative')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2B_t9P52_aLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from nltk import download\n",
        "nltk.download('punkt_tab')\n",
        "download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "positive_words = []\n",
        "negative_words = []\n",
        "for text in df['Text']:\n",
        "    # Analisando o sentimento da frase\n",
        "    sentiment_score = sia.polarity_scores(text)\n",
        "\n",
        "    # Tokenizando o texto\n",
        "    tokens = word_tokenize(text.lower())  # Convertendo para minúsculas\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]  # Remover números e stopwords\n",
        "\n",
        "    # Classificando as palavras com base na pontuação\n",
        "    if sentiment_score['compound'] >= 0.05:  # Sentimento positivo\n",
        "        positive_words.extend(tokens)\n",
        "    elif sentiment_score['compound'] <= -0.05:  # Sentimento negativo\n",
        "        negative_words.extend(tokens)\n",
        "\n",
        "# Verificando as palavras positivas e negativas\n",
        "print(f\"Palavras positivas: {positive_words}\")\n",
        "print(f\"Palavras negativas: {negative_words}\")"
      ],
      "metadata": {
        "id": "gfobb6U5F6SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerar a nuvem de palavras positivas\n",
        "positive_words_string = ' '.join(positive_words)  # Transformar a lista de palavras em uma string\n",
        "wordcloud_positive = WordCloud(width=800, height=400, background_color='white').generate(positive_words_string)\n",
        "\n",
        "# Gerar a nuvem de palavras negativas\n",
        "negative_words_string = ' '.join(negative_words)  # Transformar a lista de palavras em uma string\n",
        "wordcloud_negative = WordCloud(width=800, height=400, background_color='white').generate(negative_words_string)\n",
        "\n",
        "# Plotando as nuvens de palavras\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Nuvem de palavras positivas\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(wordcloud_positive, interpolation='bilinear')\n",
        "plt.title('Nuvem de Palavras Positivas')\n",
        "plt.axis('off')\n",
        "\n",
        "# Nuvem de palavras negativas\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(wordcloud_negative, interpolation='bilinear')\n",
        "plt.title('Nuvem de Palavras Negativas')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IvRdpx3GKs_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yvJlvLX3B_Jo"
      }
    }
  ]
}